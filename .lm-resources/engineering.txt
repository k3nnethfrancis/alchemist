#### OLD DOCUMENT ####

# Alchemist Engineering Document

## Current State
We have successfully implemented:
- Core runtime architecture for agent management
- Extension system with Discord integration
- Multi-provider ChatAgent (OpenAI, Anthropic, OpenPipe)
- Clean separation between agents, extensions, and runtime
- Graph system for building complex agent workflows
- Example workflows demonstrating the system's capabilities
- Provider-agnostic base agent with tool support

## Agent Implementation Patterns

### BaseAgent Design
The BaseAgent follows these key patterns:
1. Provider Agnostic:
   - Supports OpenAI, Anthropic, and OpenPipe
   - Uses Mirascope's provider-specific call methods
   - Handles provider-specific tool registration

2. Core Components:
   ```python
   class BaseAgent(BaseModel):
       provider: Literal["openai", "anthropic", "openpipe"]
       model: str
       persona: PersonaConfig
       history: List[BaseMessageParam]

       @prompt_template()
       def _prompt(self, query: str) -> Messages.Type:
           # Build prompt with system message, history, tools

       def _call(self, query: str) -> BaseCallResponse:
           # Make provider-specific call with tools

       async def _step(self, query: str) -> str:
           # Process conversation turn and handle tools

       async def run(self) -> None:
           # Interactive chat loop
   ```

3. Tool Integration:
   - Tools registered in provider-specific call methods
   - Async tool execution handling
   - Clean tool response formatting

### Tool Implementation Pattern
Tools follow this structure:
```python
class CustomTool(BaseTool):
    # Tool identity
    name: str = "tool_name"
    description: str = "Tool description"
    
    # Tool parameters
    param1: str = Field(..., description="Parameter description")
    param2: str = Field(default="default", description="Optional parameter")
    
    async def call(self) -> Any:
        """Tool implementation."""
        try:
            # Tool logic here
            return result
        except Exception as e:
            logger.error(f"Tool failed: {str(e)}")
            raise
```

Key Tool Principles:
1. Clear parameter definitions with Field descriptions
2. Proper async support
3. Comprehensive error handling
4. Detailed logging
5. Clean return types

### Provider-Specific Considerations
1. OpenAI:
   - Uses standard OpenAI client
   - Supports function calling format

2. Anthropic:
   - Uses Anthropic's tool format
   - Requires specific tool structure

3. OpenPipe:
   - Uses OpenAI client with OpenPipe backend
   - Maintains OpenAI compatibility

## Current Gaps and Next Steps
1. Graph System Improvements
   - Add cycle detection and handling for continuous workflows
   - Implement state persistence between cycles
   - Improve error handling and recovery
   - Add proper LLM response handling

2. Testing Improvements
   - Add graph cycle tests
   - Add error handling tests
   - Add integration tests with real LLM calls
   - Add state persistence tests
   - Ensure all existing tests pass

3. Example Updates
   - Update examples to demonstrate continuous operation
   - Add error handling examples
   - Show proper state management
   - Demonstrate cycle handling

## Project Structure
```
alchemist/
├── alchemist/             # Main package source
│   ├── ai/               # AI-related implementations
│   │   ├── agents/      # Agent implementations
│   │   │   ├── chat/    # Current implementation
│   │   │   ├── eliza/   # Next focus
│   │   │   └── terminal/ # Future implementation
│   │   ├── base/        # Base classes and interfaces
│   │   ├── graph/       # Graph implementation for building workflows
│   │   └── prompts/     # Prompt templates
│   ├── core/            # Core framework
│   │   ├── extensions/  # Extension implementations
│   │   └── mirascope/   # Mirascope integration
│   └── utils/           # Utility functions
├── examples/             # Usage examples
│   ├── chat/           
│   ├── discord/        
│   └─── graph/          
└── tests/               # Test suite
    ├── ai/
    └── core/
```

## Development Roadmap

### Phase 1: Core Infrastructure
- BaseAgent implementation with Mirascope integration
- Provider support (OpenAI, Anthropic, OpenPipe)
- Message history management
- Basic tool system
- Extension system foundation (Discord, etc.)

### Phase 2: Graph System
- Node base classes (Decision, Action, Response)
- Graph structure and workflow state
- LLM-based decision nodes
- Action node integration with tools
- Response node types
- State management and routing

### Phase 3: Eliza Implementation
- Eliza workflow definition
- Decision nodes for response analysis
- Response generation nodes
- Discord integration
- Conversation state management

### Phase 4: Long Term Memory System
- Memory storage and retrieval
- Context management
- Semantic search integration
- Memory pruning and relevance
- Memory-aware decision making

### Phase 5: Additional Agent Types
- ChatAgent improvements
- Terminal Agent
- Autonomous Agent

### Phase 6: Advanced Features
- Workflow visualization
- Debug tooling
- Testing utilities
- Documentation generation
- Additional extensions (Twitter, LinkedIn, etc.)

## Technical Requirements

### Agent Requirements
- Must implement BaseAgent interface from `ai/base/`
- Must support async operations
- Must handle conversation context
- Must implement error handling
- Must support tool usage
- Must maintain state management

### Extension Requirements
- Must implement BaseExtension interface from `core/extensions/`
- Must handle async communication
- Must support configuration management
- Must implement error handling
- Must handle rate limiting
- Must support logging

### Core System Requirements
- Must maintain separation of concerns
- Must support multiple agent types
- Must handle extension routing
- Must implement logging
- Must support configuration
- Must handle errors gracefully

## Implementation Reference
For implementation details and examples, refer to:
- `examples/graph/` - Example workflows showing graph system usage
- `examples/discord/` - Discord bot implementations
- `examples/chat/` - Basic chat agent examples

## Contributing Guidelines
1. Follow PEP 8
2. Add comprehensive docstrings
3. Include type hints
4. Write tests
5. Update documentation
6. Maintain backwards compatibility

## Mirascope Integration Guide

### Required Imports
```python
from mirascope.core import (
    BaseMessageParam,
    Messages,
    anthropic,
    openai,
    prompt_template,
)
from mirascope.core.base import BaseCallResponse
from openpipe import OpenAI as OpenPipeClient
```

### Complete Agent Implementation
```python
class BaseAgent(BaseModel):
    """Provider-agnostic chat agent for Alchemist."""

    # Core configuration
    provider: Literal["openai", "anthropic", "openpipe"]
    model: str
    persona: PersonaConfig
    history: List[BaseMessageParam] = Field(default_factory=list)

    @prompt_template()
    def _prompt(self, query: str) -> Messages.Type:
        """Build the prompt with system message, history, and tools."""
        return [
            Messages.System(
                "You are a helpful AI assistant with access to tools. "
                "Use the appropriate tool when needed."
            ),
            *self.history,
            Messages.User(query),
        ]

    def _call(self, query: str) -> BaseCallResponse:
        """Make the appropriate provider-specific call."""
        # Configure call based on provider
        if self.provider == "openai":
            call = openai.call(
                self.model, 
                tools=[ToolClass1, ToolClass2]  # List your tool classes here
            )(self._prompt)
        elif self.provider == "anthropic":
            call = anthropic.call(
                self.model,
                tools=[ToolClass1, ToolClass2]
            )(self._prompt)
        elif self.provider == "openpipe":
            call = openai.call(
                self.model,
                client=OpenPipeClient(),
                tools=[ToolClass1, ToolClass2]
            )(self._prompt)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
        
        # Execute the call
        return call(query)

    async def _step(self, query: str) -> str:
        """Process a single conversation turn."""
        # Make the call
        response = self._call(query)
        
        # Handle tool calls
        if tool := response.tool:
            try:
                result = await tool.call()
                logger.info(f"Tool executed successfully: {result}")
                return f"Tool result: {result}"
            except Exception as e:
                logger.error(f"Tool execution failed: {str(e)}")
                return f"Tool error: {str(e)}"
        
        # Handle normal responses
        self.history += [
            Messages.User(query),
            response.message_param,
        ]
        return response.content

    async def run(self) -> None:
        """Run an interactive chat session."""
        print(f"\nStarting chat with {self.provider} ({self.model})")
        while True:
            query = input("(User): ")
            if query.strip().lower() in ["exit", "quit"]:
                break
            try:
                answer = await self._step(query)
                print(f"(Assistant): {answer}")
            except Exception as e:
                logger.error(f"Error: {e}")
                print(f"[Error] {str(e)}")
```

### Tool Implementation Example
```python
class ImageGenerationTool(BaseTool):
    """Example tool for image generation."""
    
    # Tool identity (required by Mirascope)
    name: str = "generate_image"
    description: str = "Generate an image using DALL-E 3"
    
    # Tool parameters (will be filled by LLM)
    prompt: str = Field(..., description="The description of the image to generate")
    style: str = Field(
        default="Default style",
        description="Optional style guide"
    )
    
    async def call(self) -> str:
        """Tool implementation - must be async."""
        try:
            # Your tool logic here
            result = await your_async_function()
            logger.info(f"Tool success: {result}")
            return result
        except Exception as e:
            logger.error(f"Tool failed: {str(e)}")
            raise
```

### Usage Example
```python
async def main():
    # Initialize agent
    agent = BaseAgent(
        provider="openai",  # or "anthropic" or "openpipe"
        model="gpt-4o-mini",  # or appropriate model for provider
        persona=PersonaConfig(
            name="Assistant",
            role="Helpful AI assistant"
        )
    )

    # Run the agent
    await agent.run()

if __name__ == "__main__":
    asyncio.run(main())
```

### Key Implementation Notes
1. Mirascope Integration:
   - Use `@prompt_template()` for message construction
   - Return proper `Messages.Type` from _prompt
   - Use provider-specific `call()` methods with tools
   - Handle tool responses via `response.tool`

2. Provider Handling:
   - OpenAI: Direct OpenAI client usage
   - Anthropic: Uses Anthropic's format
   - OpenPipe: Uses OpenAI client with OpenPipe backend

3. Tool Requirements:
   - Must extend `BaseTool`
   - Must have `name` and `description`
   - Must implement async `call()` method
   - Parameters must use Pydantic Fields

4. Error Handling:
   - Tool errors in try/except blocks
   - Provider errors caught in run loop
   - Proper logging at all levels